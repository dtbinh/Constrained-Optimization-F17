
\subsection{Problem 1 - Quadratic Optimization}
From the problem 1 it is seen that the system can be written as:
\[H=\begin{bmatrix}
	6 & 2 & 1 \\2 & 5 & 2 \\1 & 2 & 4
\end{bmatrix}\]
\[g=\begin{bmatrix}
	-8 \\-3 \\ -3
\end{bmatrix}\]
\[A=\begin{bmatrix}
	1 & 0 \\ 0 & 1 \\ 1 & 1
\end{bmatrix}\]
\[b= \begin{bmatrix}
	3 \\ 0
\end{bmatrix}\]
Then the general KKT system can be written as:
\[\begin{bmatrix}
	H & -A \\A^T & 0
\end{bmatrix} \times \begin{bmatrix}
	x\\ \lambda
\end{bmatrix} =\begin{bmatrix}
	-g\\ b
\end{bmatrix}\]
Which in this case can be written as:
\[\begin{bmatrix}
	6 & 2 & 1 & -1 & 0\\2 & 5 & 2 & 0 & -1\\1 & 2 & 4 & -1 & -1 \\
1 & 0 & 1 & 0 & 0\\ 0 & 1 & 1 & 0 & 0
\end{bmatrix} \times \begin{bmatrix}
	x_1 \\ x_2 \\ x_3 \\ \lambda_1 \\ \lambda_2
\end{bmatrix} =\begin{bmatrix}
	8 \\3 \\ 3\\ 3 \\ 0
\end{bmatrix}\]
The system is solved in MatLab by making a function that solves the problem by use of the backslash command, see the code how it was done. The result shows that:
\[x=\begin{bmatrix}
	2 \\ -1 \\ 1
\end{bmatrix}\]
\[\lambda=\begin{bmatrix}
	3 \\ 2 
\end{bmatrix}\]
To test the solver, random values for x and $\lambda$ are generated, from these the H, g, A and b are determined. With these matrix and vectors the system are solved again. If the function finds the same x and lambda that were previously randomly chosen, the solver works. Our solver works as expected and gives the same values for x and lambda that were generated.\\
To see the sensitivity of the problem parameter sensitivity analysis is conducted. The general function is as followed. 
\[F(x,\lambda;p)= \begin{bmatrix}
	\nabla_x f(x,p)- \nabla_x c(x,p)\lambda \\ c(x,p)
\end{bmatrix}=\begin{bmatrix}
0 \\ 0
\end{bmatrix}\]
Where the implicit function theorem gives parameter sensitivities
\[[\nabla_x(p) \lambda(p)]=-[W_{xp} -\nabla_p c(x,p)\begin{bmatrix}
W_{xx} & -\nabla_x c(x,p) \\ -\nabla_x c(x,p)^T & 0
\end{bmatrix}^{-1}\]
\[ W_{xx} = \nabla^2_{xx} f(x,p) - \sum\limits_{i\in\varepsilon} \lambda_i\nabla^2_{xx}c_i(x,p)\]
\[ W_{xp} = \nabla^2_{xp} f(x,p) - \sum\limits_{i\in\varepsilon} \lambda_i\nabla^2_{xp}c_i{xp})\]
From this the sensitivity of this problem can be written as:
\[f(x,p) = \dfrac{1}{2}x^T Hx+(g+\begin{bmatrix} p_1 \\ p_2 \\ p_3 \end{bmatrix})^T x\]
\[c(x,p)=A^Tx-b-\begin{bmatrix} p_5 \\ p_6\end{bmatrix}\]
\[w_{xp}=\begin{bmatrix}
1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&0&0\\0&0&0&0&0\\\end{bmatrix}\]
\[W_{xx}=H\]
\[[\nabla x(p) \lambda(p)]= - \begin{bmatrix}H & -A^T \\-A & 0\end{bmatrix}\]
This are added to the solver and the sensitivity found are the following:
\[\nabla x(p)=\begin{bmatrix}
 	-0.0769 &  -0.0769 &  0.0769 \\
   	-0.0769 &  -0.0769 &  0.0769 \\
     0.0769 &   0.0769 & -0.0769 \\
     0.4615 &  -0.5385 &  0.5385 \\
   	-0.3846 &   0.6154 &  0.3846	
\end{bmatrix}\]
\[\lambda(p)=\begin{bmatrix}
	0.4615  & -0.3846 \\
   -0.5385  &  0.6154 \\
    0.5385  &  0.3846 \\
    2.2308  & -0.6923 \\
   -0.6923  &  3.0769 
\end{bmatrix}\]
These sensitivities can be verified by using the following equations:
\[x(p) \approx x(p_0)+\nabla_px(p_0)^T(p-p_0) \]
\[\lambda(p) \approx \lambda(p_0)+\nabla_p\lambda(p_0)^T(p-p_0) \]
These are equations(1.92) and (1.93) in Numerical Methods for Constrained Optimization by John Bagterp Joergensen.
\\
When dealing with such a quadratic problem we can always also write the dual problem. Since the Lagrangian for the primal is:
\[  L(x,\lambda) = \dfrac{1}{2} x^T H x + g^T - \mu^T (A^T x-b)\]
The dual problem is:
\[  \max_{\substack{x, \mu}}  -\dfrac{1}{2} x^T H x + b \mu^T\]
\[ s.t. Hx + g -A\mu = 0\]
This maximization problem can be converted to a minimization problem by multiplying by -1. This gives the final dual problem:
\[  \min_{\substack{x, \mu}}  \dfrac{1}{2} x^T H x - b \mu^T\]
\[ s.t. Hx + g -A\mu = 0\]
In the dual $\mu$ is the feasible point where in the primal it is x. They are related as so that the primal optimal is less or equal to the dual optimal. But if one of them have finite optima, primal optimal equals dual optimal. In some cases the dual problem can be simpler and therefore easier to solve the primal. 
